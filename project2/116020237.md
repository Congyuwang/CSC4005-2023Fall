# Report

116020237

Congyu Wang

## Compile & Execution

Compile command:
```sh
cmake -B build
cmake --build build --parallel 4
```

Execution command:
```sh
./src/sbach.sh
```

Full execution command:
```sh
screen -L python3 ./src/test_and_perf.py
```
This will:
1. Run all programs using `srun`.
2. Test their correctness by comparing outputs.
3. Perf (basic and detailed) and save data to `./profiling`.
4. Save console output to `screenlog.0` file.

## Optimizations & Performance

### Performance Summary

| implementation                    | Matrices 2048*2048 |
| :-- | :--: |
| naive (original)                  | 81855 ms            |
| naive (inline + no bound check)   | 43848 ms            |
| locality                          | 6378  ms            |
| SIMD (AVX2)                       | 2341  ms            |
| SIMD aligned (AVX2)               | 2204  ms            |
| SIMD tiled (16*16 AVX512)         | 871   ms            |
| SIMD tiled aligned (16*16 AVX512) | 804   ms            |
| SIMD tiled aligned + openmp 32    | 99    ms            |
| SIMD openmp mpi(ncpu=8 nproc=4)   | 77    ms            |

### naive_old (81855 ms)

The original naive implementation.

### naive (43848 ms)

Optimized `operator[]` by:
- removing bound check
- inline function

```diff
+ inline
  int*
  Matrix::operator[](size_t rowIndex)
  {
-   if (rowIndex < rows) {
      return data[rowIndex];
-   } else {
-     throw std::out_of_range("Row index out of range");
-   }
  }
```

This makes the same naive implementation about 2x faster.

Note: this change shouldn't make much difference for
later implementations, since other implementations
use the operator only in outer loops.

### Locality (6378 ms)

Changed loop order. Based on `naive (43848 ms)`, about x7 times faster.

### SIMD AVX2 (2341 ms)

Based on `Locality`, about x3 times faster.

An interesting issue about the generated assembly code:
[why-doesnt-gcc-resolve-mm256-loadu-pd-as-single-vmovupd](https://stackoverflow.com/questions/52626726/why-doesnt-gcc-resolve-mm256-loadu-pd-as-single-vmovupd).
Added flag `-mno-avx256-split-unaligned-load` and `-mno-avx256-split-unaligned-store` to solve
the gcc suboptimal assembly generation.

### SIMD Aligned AVX2 (2204 ms)

Aligned matrix arrays to 256bits / 512bits,
to use `load/store` instead of `loadu/storeu`.

This creates a ≈10% overall speed up.

### SIMD AVX512 TILED16 (871 ms)

Tiled loop to reduce CPU-RAM bus traffic. The tile size is 16*16.

For implementation convenience, AVX512 is used to contain
a single row of each tile (16 int32).

AVX512 + TILED is about 3x times faster compared to AVX2 alone.

### SIMD Aligned AVX512 TILED16 (804 ms)

Aligned matrix arrays to 256bits / 512bits,
to use `load/store` instead of `loadu/storeu`.

This creates a ≈10% overall speed up.

### OpenMP 32 + SIMD Aligned AVX512 TILED16 (99 ms)

OpenMP 32 threads, about 8x times faster by threading.

### MPI with 4 process * 8 threads (77 ms)

Multiple processes with same total number of threads.
Runs faster than just threading perhaps due to scheduling.

## Detailed Perf

### naive_old vs. naive

The original version of naive_old has **310K** cache misses,
compared to **186K** cache misses after removing bound checking
and function inline, perhaps due to more complex control flow.

### naive vs. locality

naive (190K Samples):

```
Samples│
   144 │402d70:   mov    (%r8,%rax,8),%rdx  │   load matrix2[k]
141473 │402d74:   mov    (%rdx,%rdi,1),%edx │   load matrix2[k][j]
 22382 │402d77:   imul   (%r9,%rax,4),%edx  │ * load matrix1[i][k]
     2 │402d7c:   add    $0x1,%rax          │   k++
  5405 │402d80:   add    %edx,%ecx          │ +
       │402d82:   cmp    %rax,%rbx          │   k < K
 17467 │402d85:   mov    %ecx,(%rsi)        │   store result[i][j]
     2 │402d87: → jne    402d70             │
```

locality (30K Samples):

```
Samples│
 14093 │402d70:   mov    (%rdi,%rax,4),%ecx  │   load matrix2[k][j]
   104 │402d73:   imul   %esi,%ecx           │ *      matrix2[i][k]
 12874 │402d76:   add    %ecx,(%rdx,%rax,4)  │ + store result[i][j]
    19 │402d79:   add    $0x1,%rax           │   j++
    20 │402d7d:   cmp    %rax,%rbx           │   j < N
    14 │402d80: → jne    402d70              │
```

Loading `matrix2[k][j]` alone takes `141K` out of `190K` samples.
Since we are looping over `k`, the memory access pattern is slow.

### simd vs simd-aligned

simd (13K Samples):
```
Samples│
  6374 │402d88:   vpmulld (%rax,%rdx,4),%ymm1,%ymm0 │ * matrix2[k][j..]
  1379 │402d8e:   vpaddd (%rcx,%rdx,4),%ymm0,%ymm0  │ + result[i][j..]
  1276 │402d93:   vmovdqu %ymm0,(%rcx,%rdx,4)       │ s result[i][j..]
    12 │402d98:   add    $0x8,%rdx                  │   j += 8
     2 │402d9c:   cmp    %rdx,%rbx                  │   j < N
   697 │402d9f: → ja     402d88                     │
```

simd aligned (12K Samples):
```
Samples│
  6071 │402d88:   vpmulld (%rax,%rdx,4),%ymm1,%ymm0 │ * matrix2[k][j..]
  1436 │402d8e:   vpaddd (%rcx,%rdx,4),%ymm0,%ymm0  │ + result[i][j..]
  1049 │402d93:   vmovdqa %ymm0,(%rcx,%rdx,4)       │ s result[i][j..]
    15 │402d98:   add    $0x8,%rdx                  │   j += 8
     6 │402d9c:   cmp    %rdx,%rbx                  │   j < N
   718 │402d9f: → ja     402d88                     │
```

`load` and `store` are both faster for aligned memory.

## GPU Implementation

run `python3 tests.py`.

| implementation | Matrices 1024*1024 | Matrices 2048*2048 |
| :-- | :--: | :--: |
| OpenACC        | 8 ms               | 66 ms              |
| CUDA           | 6.7 ms             | 46 ms              |
